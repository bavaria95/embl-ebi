The very first problem which I can see in this solution is the resulting matrix of cooperation. I don't know actual order of magnitude of different authors in those 22M of articles, but if we mark that number as N, we'll need N^2 of the memory just for that matrix.
For the sake of simplicity, let's say that there are 1M of distinct authors. So, there will be 10^12 cells in the matrix. If assume, that we are going to store those cells as integers which occupy 4 bytes (not even mentioning that my solution uses python lists, and apparently uses memory even in less efficient way) this matrix will require around 4 terabytes (if I haven't made a mistake in my calculations) of RAM memory, which is rather impossible to achieve on a single machine nowadays.
Possible solutions might include storing this matrix as a sparse matrix (intuitively, it will be sparse indeed) or distributing matrix over number of machines (e.g. by rows or columns).

Then the next issue I can see is processing this XML file. In the example file it's around 350 kB per article, which will extrapolate in ~8 GB XML file. Although, I think that actual Medline database contains more information than provided in the example, so it can be a couple of times more. Albeit, it seems to be rather possible to allocate memory in RAM for opening such file even on a single machine, parsing its' XML structure requires way more space and it may create problems on most of the machines. Thus, this problem can be quite easily be solved by using batch processing (e.g. hadoop)